{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec2fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /home/lab_hardik/my_conda/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "import os, xarray as xr, numpy as np, pandas as pd, dask, random, netCDF4\n",
    "import cartopy.crs as ccrs, matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from datetime import datetime as dt, timedelta\n",
    "import datetime, time\n",
    "\n",
    "import importlib\n",
    "import matplotlib.patches as mpatches\n",
    "import gc\n",
    "from matplotlib.gridspec import GridSpec\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": False})\n",
    "import sys\n",
    "sys.path.insert(1, '/home/data/lab_hardik/udf/')\n",
    "\n",
    "import h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2316a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30. 60. 90.]\n",
      "[10. 30. 50.]\n",
      "$\\mathregular{\\mathcal{S}'_{x}}$\n",
      "[\"$u'$\", \"$v'$\", \"$w'$\", \"$\\\\mathcal{S}'_x$\", \"$\\\\mathcal{S}'_y$\", \"$\\\\mathcal{S}'_z$\", '${u̅}$', '${v̅}$', '${w̅}$', '$\\\\mathcal{S}̅_x$', '$\\\\mathcal{S}̅_y$', '$\\\\mathcal{S}̅_z$']\n",
      "dict_values([\"$u'$$\\\\mathcal{S}'_x$\", \"$v'$$\\\\mathcal{S}'_y$\", \"$w'$$\\\\mathcal{S}'_z$\", \"$u'$$\\\\mathcal{S}̅_x$\", \"$v'$$\\\\mathcal{S}̅_y$\", \"$w'$$\\\\mathcal{S}̅_z$\", \"${u̅}$$\\\\mathcal{S}'_x$\", \"${v̅}$$\\\\mathcal{S}'_y$\", \"${w̅}$$\\\\mathcal{S}'_z$\", '${u̅}$$\\\\mathcal{S}̅_x$', '${v̅}$$\\\\mathcal{S}̅_y$', '${w̅}$$\\\\mathcal{S}̅_z$'])\n",
      "[\"$v'$$\\\\mathcal{S}̅_y$\", \"$w'$$\\\\mathcal{S}̅_z$\", \"${u̅}$$\\\\mathcal{S}'_x$\"]\n",
      "[\"$u'$$\\\\mathcal{S}'_x$\", \"$v'$$\\\\mathcal{S}'_y$\", \"$w'$$\\\\mathcal{S}'_z$\"]\n",
      "['${u̅}$$\\\\mathcal{S}̅_x$', '${v̅}$$\\\\mathcal{S}̅_y$', '${w̅}$$\\\\mathcal{S}̅_z$']\n",
      "[\"$u'$$\\\\mathcal{S}̅_x$\", \"${v̅}$$\\\\mathcal{S}'_y$\", \"${w̅}$$\\\\mathcal{S}'_z$\"]\n",
      "[30. 60. 90.]\n",
      "[10. 30. 50.]\n"
     ]
    }
   ],
   "source": [
    "import composite_funcs\n",
    "import grad_funcs\n",
    "importlib.reload(composite_funcs)\n",
    "from composite_funcs import *\n",
    "from grad_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a202f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out = '/home/data/lab_hardik/heatwaves/ERA5/analyses/phase_relations/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a440f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_dir = '/home/data/lab_hardik/heatwaves/ERA5/dtree_anoms/processed_files/'\n",
    "# files = [s for s in os.listdir(dat_dir) if 'Apr' in s]\n",
    "# # files.sort(key = os.path.getctime)\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5ba392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30. 60. 90.]\n",
      "[10. 30. 50.]\n"
     ]
    }
   ],
   "source": [
    "# tst = xr.open_dataset('/home/data/lab_hardik/data/ERA5/climatology/t_3D/dly_clmtlgy_stdPlevs_prmnsn_temperature_1980_2022_wFeb.nc')\n",
    "# tst.close()\n",
    "\n",
    "importlib.reload(composite_funcs)\n",
    "from composite_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8b4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5246\n",
      "['era5_u_component_of_wind_1980_3_1.nc', 'era5_u_component_of_wind_1980_3_2.nc', 'era5_u_component_of_wind_1980_3_3.nc', 'era5_u_component_of_wind_1980_3_4.nc', 'era5_u_component_of_wind_1980_3_5.nc', 'era5_u_component_of_wind_1980_3_6.nc']\n",
      "5246\n",
      "['era5_v_component_of_wind_1980_3_1.nc', 'era5_v_component_of_wind_1980_3_2.nc', 'era5_v_component_of_wind_1980_3_3.nc', 'era5_v_component_of_wind_1980_3_4.nc', 'era5_v_component_of_wind_1980_3_5.nc', 'era5_v_component_of_wind_1980_3_6.nc']\n",
      "5246\n",
      "['era5_temperature_1980_3_1.nc', 'era5_temperature_1980_3_2.nc', 'era5_temperature_1980_3_3.nc', 'era5_temperature_1980_3_4.nc', 'era5_temperature_1980_3_5.nc', 'era5_temperature_1980_3_6.nc']\n",
      "3416\n",
      "['era5_geopotential_1995_3_1.nc', 'era5_geopotential_1995_3_2.nc', 'era5_geopotential_1995_3_3.nc', 'era5_geopotential_1995_3_4.nc', 'era5_geopotential_1995_3_5.nc', 'era5_geopotential_1995_3_6.nc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prepare_files(variable='u') # 1980 to 2022\n",
    "prepare_files(variable='v') # 1980 to 2022\n",
    "prepare_files(variable='t') # 1980 to 2022\n",
    "prepare_files(variable='z') # 1995 to 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e006fae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['era5/.zgroup', 'era5/.zmetadata', 'era5/era5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab_hardik/my_conda/lib/python3.9/site-packages/paramiko/pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "/home/lab_hardik/my_conda/lib/python3.9/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "/home/lab_hardik/my_conda/lib/python3.9/site-packages/paramiko/transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "/home/data/lab_hardik/udf/composite_funcs.py:62: UserWarning: Converting non-nanosecond precision datetime values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time.\n"
     ]
    }
   ],
   "source": [
    "ds_z_1980_1994 = prepare_z_ds(year_min = 1980, year_max = 1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ff08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_z_1980_1994 = \\\n",
    "ds_z_1980_1994.sel(isobaricInhPa = slice(600,900))\n",
    "# .rechunk({'isobaricInhPa':1, 'longitude':720})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a986f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f36f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd2e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import variables\n",
    "from variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c158918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/data/lab_hardik/heatwaves/ERA5/dtree_anoms/processed_files/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93eb4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_M(ds = xr.Dataset()): #ds_brnch_path1\n",
    "    \n",
    "    R = 287\n",
    "    Rearth = 6378*10**3\n",
    "    steps_x = 4*10\n",
    "    del_lambda = 0.25*np.pi/180\n",
    "    del_phi = 0.25*np.pi/180\n",
    "\n",
    "    ds = compute_zdiff_fields(ds)\n",
    "    ds['density'] = (ds['isobaricInhPa']*100)/(287*ds['t'])\n",
    "\n",
    "    Axy_2 = Rearth**2*del_lambda*(\n",
    "        np.sin((Rlat.latitude + 0.25)*np.pi/180) - np.sin(Rlat.latitude*np.pi/180)\n",
    "    )\n",
    "    M = (Axy_2*ds['z_diff_down']/g*.5*(ds['density'] + ds['density'].shift(isobaricInhPa=-1)))\\\n",
    "    .sel(latitude=slice(24,30.75), longitude=slice(68,77.75))\n",
    "    \n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06332732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(ds, save_str = ''):\n",
    "#     global fig, ax\n",
    "    # num_plots = 10 if df.shape[0]>=10 else df.shape[0]\n",
    "    # num_plots = 10 if len(ds.date)>=10 else len(ds.date)\n",
    "\n",
    "    # num_plots = int(np.ceil(len(ds.date)/2)*2)\n",
    "\n",
    "    num_plots = len(ds.date)\n",
    "\n",
    "    nrow = int(np.ceil(num_plots/2))\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        nrow,2, figsize=(16, nrow*4), sharey=True, subplot_kw={'projection': ccrs.PlateCarree()}\n",
    "    )\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        r = bicol_panel_rownum(i)\n",
    "        c = bicol_panel_colnum(i)\n",
    "        print('rowunm',r)\n",
    "        print('colnum',c)\n",
    "\n",
    "        tot_flx = df.Net_Adv.iloc[i].round(0)\n",
    "        dse_anom = df.dse_anom.iloc[i].round(0)\n",
    "\n",
    "        da_plt = ds[dse_var].isel(date = i)#.mean(['date'])#.compute()\n",
    "        da_cont = ds[z_var].isel(date = i)#.mean(['date'])#.compute()\n",
    "        da_u = (ds[u_var]).isel(date = i)#.mean(['date'])#.compute()\n",
    "        da_v = (ds[v_var]).isel(date = i)#.mean(['date'])#.compute()\n",
    "        # da_u = ds['adv_flx_x'].mean(['date']).compute()\n",
    "        # da_v = ds['adv_flx_y'].mean(['date']).compute()\n",
    "\n",
    "        # vmin = np.nan; vmax=np.nan\n",
    "        inner_plot_composite(\n",
    "            da_plt = da_plt, da_cont = da_cont, da_u = da_u, da_v = da_v, xmin=0, xmax=120, ymin=10, ymax=65, \n",
    "            cont_divisions=20, coarsen_pts=8,\n",
    "            t_type = 'dse_anom',scale_quiv= 70, \n",
    "            i = r, j = c, ax=ax[r,c], \n",
    "            title_str = 'dse_anom = {}, dse_flx = {}, date = {}'.format(dse_anom, tot_flx, str(df.date.iloc[i].date()))\n",
    "            )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_str != '':\n",
    "        plt.savefig('/home/data/lab_hardik/heatwaves/ERA5/analyses/phase_relations/{}'.format(save_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cee198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "def starts_with_substring_and_number(s, substring): \n",
    "    pattern = rf\"^{re.escape(substring)}\\d+\" \n",
    "    return re.match(pattern, s) is not None \n",
    "# Example usage \n",
    "string = \"prefix12345\" \n",
    "substring = \"prefix\" \n",
    "result = starts_with_substring_and_number(string, substring) \n",
    "print(result) \n",
    "# Output: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d99c3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_comp(dates = list(), sample=np.nan, two_day_averaging = True):\n",
    "    global ds_z_comp, ds_v_comp, ds_u_comp, ds_t_comp, brnch_dates, brnch_dates2\n",
    "#     brnch_dates = df.date.sample(10 if df.shape[0] >= 10 else df.shape[0])\n",
    "    print('sample', sample)\n",
    "\n",
    "    if sample != sample: \n",
    "        print('sample == np.nan')\n",
    "        brnch_dates = dates\n",
    "    else: \n",
    "        brnch_dates = random.sample(dates, sample if df.shape[0] >= sample else df.shape[0])\n",
    "\n",
    "    print('len(brnch_dates)', len(brnch_dates))\n",
    "\n",
    "    if two_day_averaging == True: \n",
    "        brnch_dates2 = ([s - timedelta(days=1) for s in brnch_dates] + brnch_dates)\n",
    "        print('len(brnch_dates2)', len(brnch_dates2))\n",
    "    else: \n",
    "        brnch_dates2 = brnch_dates\n",
    "    \n",
    "    press = slice(900,600)\n",
    "\n",
    "    ds_z_comp = filt_comp_files(\n",
    "        variable='z', dates = brnch_dates2, press=press, \n",
    "        ds_z_1980_1994 = ds_z_1980_1994.sortby('isobaricInhPa', ascending=False), \n",
    "        ymin=10, ymax=60, xmin=0, xmax=120\n",
    "    )\n",
    "    \n",
    "    ds_v_comp = filt_comp_files(variable='v', dates = brnch_dates2, press= press, ymin=10, ymax=60, xmin=0, xmax=120)\n",
    "    ds_u_comp = filt_comp_files(variable='u', dates = brnch_dates2, press=press, ymin=10, ymax=60, xmin=0, xmax=120)\n",
    "    ds_t_comp = filt_comp_files(variable='t', dates = brnch_dates2, press=press, ymin=10, ymax=60, xmin=0, xmax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "587aa11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anoms():\n",
    "    global ds_z_comp, ds_v_comp, ds_u_comp, ds_t_comp\n",
    "    \n",
    "    press = slice(900,600)\n",
    "    ds_z_comp = create_anoms(variable='z', press=press, ds=ds_z_comp, clmt_file_map = rolling_clmt_file_map)\n",
    "    ds_t_comp = create_anoms(variable='t', press=press, ds=ds_t_comp, clmt_file_map = rolling_clmt_file_map)\n",
    "    ds_u_comp = create_anoms(variable='u', press=press, ds=ds_u_comp, clmt_file_map = rolling_clmt_file_map)\n",
    "    ds_v_comp = create_anoms(variable='v', press=press, ds=ds_v_comp, clmt_file_map = rolling_clmt_file_map)\n",
    "\n",
    "    ds_t_comp['t_clmt'] = ds_t_comp['t'] - ds_t_comp['t_anom_10D']\n",
    "    ds_z_comp['z_clmt'] = ds_z_comp['z'] - ds_z_comp['z_anom_10D']\n",
    "\n",
    "    ds_u_comp['u_clmt'] = ds_u_comp['u'] - ds_u_comp['u_anom_10D']\n",
    "    ds_v_comp['v_clmt'] = ds_v_comp['v'] - ds_v_comp['v_anom_10D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8bc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_branch_data(two_day_averaging = True):\n",
    "    ymin=10\n",
    "    ymax=60\n",
    "    xmin=0\n",
    "    xmax=120\n",
    "    \n",
    "    global ds_v_comp, ds_u_comp, ds_z_comp, ds_t_comp, ds_brnch_path1, ds_brnch, ds_brnch2, da_dse_anom_Lag1\n",
    "    \n",
    "    ds_brnch_path1 = xr.combine_by_coords(\n",
    "        [ds_v_comp, ds_u_comp, ds_z_comp, ds_t_comp], \n",
    "        join='inner', combine_attrs='drop_conflicts'\n",
    "    )\\\n",
    "    .sel(latitude=slice(ymax,ymin), longitude=slice(xmin,xmax)).chunk({'latitude':241,'longitude':481})\n",
    "\n",
    "    ds_brnch_path1['density'] = (ds_brnch_path1['isobaricInhPa']*100)/(287*ds_brnch_path1['t'])\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "#     Rlat = Rlat_fn(ds = ds_brnch_path1)\n",
    "#     M = compute_M(ds = ds_brnch_path1)\n",
    "\n",
    "    ds_brnch = ds_brnch_path1.weighted(ds_brnch_path1['density']).mean('isobaricInhPa').sortby(['latitude','longitude','date'])\n",
    "    \n",
    "#     for variable in list(ds_brnch.data_vars):\n",
    "#         print(variable)\n",
    "#         ds_brnch[variable] = vert_MassAve_qty(\n",
    "#             M = M, \n",
    "#             qty = ds_brnch[variable], \n",
    "#             p_low=600, p_high=900\n",
    "#         ).compute()\n",
    "    \n",
    "    print('ds_brnch.dims', ds_brnch.dims)\n",
    "\n",
    "    ds_brnch['dse'] = 1005*ds_brnch['t'] + ds_brnch['z']\n",
    "    ds_brnch['dse_anom'] = 1005*ds_brnch['t_anom_10D'] + ds_brnch['z_anom_10D']\n",
    "    ds_brnch['dse_clmt'] = 1005*ds_brnch['t_clmt'] + ds_brnch['z_clmt']\n",
    "    ds_brnch['dse_anom_Lag1'] = ds_brnch['dse_anom'].shift(date=1)\n",
    "    \n",
    "    da_dse_anom_Lag1 = ds_brnch['dse_anom_Lag1'].sel(date = brnch_dates)\n",
    "\n",
    "    if two_day_averaging == True:\n",
    "        ds_brnch = ds_brnch.sortby('date')\n",
    "        ds_brnch2 = 0.5*(ds_brnch + ds_brnch.shift(date = 1))\n",
    "\n",
    "        ds_brnch2 = xr.merge(\n",
    "            [\n",
    "                ds_brnch2[[s for s in list(ds_brnch2.data_vars) if 'Lag1' not in s]].sel(date = brnch_dates), \n",
    "                da_dse_anom_Lag1, \n",
    "\n",
    "            ])\n",
    "\n",
    "        print('len(ds_brnch.date)',len(ds_brnch.date))\n",
    "        print('len(ds_brnch2.date)',len(ds_brnch2.date))\n",
    "        \n",
    "#         da_z_tst0 = client.scatter(ds_brnch['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[0]))\n",
    "#         da_z_tst1 = client.scatter(ds_brnch['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[1]))\n",
    "#         da_z_tst2 = client.scatter(ds_brnch2['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[1]))\n",
    "\n",
    "#         def myfunc(dask_array):\n",
    "#             return dask_array.compute()\n",
    "\n",
    "#         future0 = client.submit(myfunc, da_z_tst0)\n",
    "#         a0 = client.gather(future0)\n",
    "\n",
    "        a0 = ds_brnch['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[0]).values\n",
    "        a1 = ds_brnch['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[1]).values\n",
    "        a2 = ds_brnch2['z_anom_10D'].isel(latitude=20, longitude=20).sel(date = ds_brnch.date[1]).values\n",
    "        \n",
    "#         future1 = client.submit(myfunc, da_z_tst1)\n",
    "#         a1 = client.gather(future1)# ds_brnch['z_anom_10D'].sel(date = ds_brnch.date[1])[20,20].values\n",
    "        \n",
    "#         future2 = client.submit(myfunc, da_z_tst2)\n",
    "#         a2 = client.gather(future2) #ds_brnch2['z_anom_10D'].sel(date = ds_brnch.date[1])[20,20].values\n",
    "\n",
    "        print(0.5*(a0+a1))\n",
    "        print(a2)\n",
    "\n",
    "        if np.round(0.5*(a0+a1),3) != np.round(a2,3):\n",
    "            return 'incorrect 2D ave'\n",
    "    else:\n",
    "        ds_brnch2 = ds_brnch\n",
    "\n",
    "    print(ds_brnch2.dims)\n",
    "    print(ds_brnch2.data_vars)\n",
    "    \n",
    "#     del ds_v_comp, ds_u_comp, ds_z_comp, ds_t_comp\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    \n",
    "    return ds_brnch, ds_brnch2\n",
    "\n",
    "def write_nc(ds, dir_out = dir_out, file_out = str):\n",
    "    ds.to_netcdf(dir_out + file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43368ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af76cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30. 60. 90.]\n",
      "[10. 30. 50.]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(grad_funcs)\n",
    "from grad_funcs import *\n",
    "\n",
    "importlib.reload(composite_funcs)\n",
    "from composite_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fa32850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/data/lab_hardik/heatwaves/ERA5/dtree_anoms/processed_files/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "404da4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Apr_posbg_decay_nl2_yNL_v2.csv']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files0 = [s for s in os.listdir(dat_dir) if s.startswith('Apr_pos') and 'v2' in s \n",
    "#          and 'nl_' not in s # and 'nl2' not in s\n",
    "#         and 'amplif_nl_xNL' not in s # this was from an old run\n",
    "#          and 'posbg2_decay_nl_xNL' not in s # posbg2 is included in posbg\n",
    "#         ]\n",
    "\n",
    "# files0 = [\"Apr_posbg_decay_nl2_yNL_v2.csv\"]\n",
    "\n",
    "files0 = [\"Apr_posbg_decay_nl1_xNL_v2.csv\"]\n",
    "files0 = ['Apr_posbg_decay_nlsat_yNL_v2.csv']\n",
    "files0=[\"Apr_posbg_decay_nl2_yNL_v2.csv\"]\n",
    "files0.sort(key = lambda s: (s.split('_')[1], s.split('_')[2], s.split('_')[3]))\n",
    "print(len(files0))\n",
    "\n",
    "files0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0943b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1cd42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [s for s in files0 if s.endswith(\"decay_ql_xyip_v2.csv\") or s.endswith(\"decay_nl1_yNL_v2.csv\")]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83546e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38a43133",
   "metadata": {},
   "source": [
    "## New data prep section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd9c7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(composite_funcs)\n",
    "from composite_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36ae19b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>$u'$$\\mathcal{S}'_x$</th>\n",
       "      <th>$v'$$\\mathcal{S}'_y$</th>\n",
       "      <th>$w'$$\\mathcal{S}'_z$</th>\n",
       "      <th>$v'$$\\mathcal{S}̅_y$</th>\n",
       "      <th>$w'$$\\mathcal{S}̅_z$</th>\n",
       "      <th>${u̅}$$\\mathcal{S}'_x$</th>\n",
       "      <th>T_main</th>\n",
       "      <th>NL_sum</th>\n",
       "      <th>x_contrib_NL</th>\n",
       "      <th>...</th>\n",
       "      <th>z_contrib_NL</th>\n",
       "      <th>Net_Adv</th>\n",
       "      <th>$\\mathcal{S}'_y$</th>\n",
       "      <th>dse_anom</th>\n",
       "      <th>dse_anom_Lag1</th>\n",
       "      <th>$u'$</th>\n",
       "      <th>$v'$</th>\n",
       "      <th>$w'$</th>\n",
       "      <th>$\\mathcal{S}'_x$</th>\n",
       "      <th>$\\mathcal{S}'_z$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-18</td>\n",
       "      <td>-447.920768</td>\n",
       "      <td>-107.081774</td>\n",
       "      <td>-10.163428</td>\n",
       "      <td>-110.394913</td>\n",
       "      <td>893.855788</td>\n",
       "      <td>-783.229455</td>\n",
       "      <td>0.231421</td>\n",
       "      <td>-565.165970</td>\n",
       "      <td>79.254731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.798309</td>\n",
       "      <td>-705.801441</td>\n",
       "      <td>-0.002151</td>\n",
       "      <td>110.929630</td>\n",
       "      <td>119.192018</td>\n",
       "      <td>1.781251</td>\n",
       "      <td>-0.590410</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>-0.232946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>-491.043593</td>\n",
       "      <td>-1042.244893</td>\n",
       "      <td>370.839153</td>\n",
       "      <td>1965.431873</td>\n",
       "      <td>-341.261646</td>\n",
       "      <td>-1502.587454</td>\n",
       "      <td>121.582773</td>\n",
       "      <td>-1162.449333</td>\n",
       "      <td>25.788376</td>\n",
       "      <td>...</td>\n",
       "      <td>19.475541</td>\n",
       "      <td>-936.367250</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>825.353184</td>\n",
       "      <td>2619.605998</td>\n",
       "      <td>1.062119</td>\n",
       "      <td>4.238345</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>-0.282281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-04-29</td>\n",
       "      <td>-214.704113</td>\n",
       "      <td>-956.098205</td>\n",
       "      <td>85.732790</td>\n",
       "      <td>551.894457</td>\n",
       "      <td>241.836896</td>\n",
       "      <td>-702.619547</td>\n",
       "      <td>91.111806</td>\n",
       "      <td>-1085.069528</td>\n",
       "      <td>17.086997</td>\n",
       "      <td>...</td>\n",
       "      <td>6.822952</td>\n",
       "      <td>-752.598533</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>-171.628959</td>\n",
       "      <td>414.428641</td>\n",
       "      <td>-0.065348</td>\n",
       "      <td>2.632642</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.047313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-04-13</td>\n",
       "      <td>-276.302201</td>\n",
       "      <td>-1106.881364</td>\n",
       "      <td>48.271706</td>\n",
       "      <td>797.552748</td>\n",
       "      <td>242.269162</td>\n",
       "      <td>-329.868994</td>\n",
       "      <td>709.952915</td>\n",
       "      <td>-1334.911859</td>\n",
       "      <td>19.302189</td>\n",
       "      <td>...</td>\n",
       "      <td>3.372212</td>\n",
       "      <td>-754.770886</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>793.372485</td>\n",
       "      <td>1680.129163</td>\n",
       "      <td>-2.183205</td>\n",
       "      <td>2.395116</td>\n",
       "      <td>-0.002121</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.080869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-04-29</td>\n",
       "      <td>371.138501</td>\n",
       "      <td>-1719.281838</td>\n",
       "      <td>69.068122</td>\n",
       "      <td>-1117.687922</td>\n",
       "      <td>536.627849</td>\n",
       "      <td>875.686852</td>\n",
       "      <td>294.626779</td>\n",
       "      <td>-1279.075215</td>\n",
       "      <td>17.186408</td>\n",
       "      <td>...</td>\n",
       "      <td>3.198356</td>\n",
       "      <td>-1015.545108</td>\n",
       "      <td>-0.003942</td>\n",
       "      <td>993.028169</td>\n",
       "      <td>1703.129178</td>\n",
       "      <td>3.530700</td>\n",
       "      <td>-5.572218</td>\n",
       "      <td>-0.004012</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>-0.045051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>86.293640</td>\n",
       "      <td>-1362.389329</td>\n",
       "      <td>232.801511</td>\n",
       "      <td>1378.579970</td>\n",
       "      <td>-669.498137</td>\n",
       "      <td>-306.865102</td>\n",
       "      <td>402.216732</td>\n",
       "      <td>-1043.294178</td>\n",
       "      <td>5.131991</td>\n",
       "      <td>...</td>\n",
       "      <td>13.844999</td>\n",
       "      <td>-832.042123</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>2741.316962</td>\n",
       "      <td>4043.701789</td>\n",
       "      <td>-1.836591</td>\n",
       "      <td>3.690447</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>-0.282537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  $u'$$\\mathcal{S}'_x$  $v'$$\\mathcal{S}'_y$  \\\n",
       "0 2018-04-18           -447.920768           -107.081774   \n",
       "1 2007-04-01           -491.043593          -1042.244893   \n",
       "2 2011-04-29           -214.704113           -956.098205   \n",
       "3 2003-04-13           -276.302201          -1106.881364   \n",
       "4 2002-04-29            371.138501          -1719.281838   \n",
       "5 2022-04-12             86.293640          -1362.389329   \n",
       "\n",
       "   $w'$$\\mathcal{S}'_z$  $v'$$\\mathcal{S}̅_y$  $w'$$\\mathcal{S}̅_z$  \\\n",
       "0            -10.163428           -110.394913            893.855788   \n",
       "1            370.839153           1965.431873           -341.261646   \n",
       "2             85.732790            551.894457            241.836896   \n",
       "3             48.271706            797.552748            242.269162   \n",
       "4             69.068122          -1117.687922            536.627849   \n",
       "5            232.801511           1378.579970           -669.498137   \n",
       "\n",
       "   ${u̅}$$\\mathcal{S}'_x$      T_main       NL_sum  x_contrib_NL  ...  \\\n",
       "0             -783.229455    0.231421  -565.165970     79.254731  ...   \n",
       "1            -1502.587454  121.582773 -1162.449333     25.788376  ...   \n",
       "2             -702.619547   91.111806 -1085.069528     17.086997  ...   \n",
       "3             -329.868994  709.952915 -1334.911859     19.302189  ...   \n",
       "4              875.686852  294.626779 -1279.075215     17.186408  ...   \n",
       "5             -306.865102  402.216732 -1043.294178      5.131991  ...   \n",
       "\n",
       "   z_contrib_NL      Net_Adv  $\\mathcal{S}'_y$     dse_anom  dse_anom_Lag1  \\\n",
       "0      1.798309  -705.801441         -0.002151   110.929630     119.192018   \n",
       "1     19.475541  -936.367250          0.000837   825.353184    2619.605998   \n",
       "2      6.822952  -752.598533          0.002141  -171.628959     414.428641   \n",
       "3      3.372212  -754.770886          0.005019   793.372485    1680.129163   \n",
       "4      3.198356 -1015.545108         -0.003942   993.028169    1703.129178   \n",
       "5     13.844999  -832.042123          0.004367  2741.316962    4043.701789   \n",
       "\n",
       "       $u'$      $v'$      $w'$  $\\mathcal{S}'_x$  $\\mathcal{S}'_z$  \n",
       "0  1.781251 -0.590410 -0.005415          0.001766         -0.232946  \n",
       "1  1.062119  4.238345  0.001237          0.003351         -0.282281  \n",
       "2 -0.065348  2.632642 -0.001481          0.001637          0.047313  \n",
       "3 -2.183205  2.395116 -0.002121          0.000908         -0.080869  \n",
       "4  3.530700 -5.572218 -0.004012         -0.002100         -0.045051  \n",
       "5 -1.836591  3.690447  0.003416          0.000602         -0.282537  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(files[0], parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "599aaf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/data/lab_hardik/heatwaves/ERA5/analyses/phase_relations/data/\n",
      "Apr_posbg_decay_nl2_yNL_v2.csv\n",
      "out of 6 nc files,\n",
      "6 files exist\n",
      "6 files exist\n"
     ]
    }
   ],
   "source": [
    "print(dir_out)\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    print(\"out of\", pd.read_csv(file, parse_dates=['date']).shape[0], \"nc files,\")\n",
    "    \n",
    "    existing_nc_lst = [\n",
    "        s for s in os.listdir(dir_out) if \n",
    "        starts_with_substring_and_number(s, file.replace('.csv', '')+'_')\n",
    "        and s.endswith('_2Dave.nc')\n",
    "    ]\n",
    "    \n",
    "    print(len(existing_nc_lst), 'files exist')\n",
    "    \n",
    "#     [os.remove(dir_out + existing_nc_lst[i]) for i in range(len(existing_nc_lst))]\n",
    "    \n",
    "    existing_nc_lst = [\n",
    "        s for s in os.listdir(dir_out) if \n",
    "        starts_with_substring_and_number(s, file.replace('.csv', '')+'_')\n",
    "        and s.endswith('_2Dave.nc')\n",
    "    ]\n",
    "    print(len(existing_nc_lst), 'files exist')\n",
    "#     print(existing_nc_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed56c6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import read_data_DSE_Flux_Trees_1980_2022\n",
    "from read_data_DSE_Flux_Trees_1980_2022 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e1c452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2010 = \\\n",
    "# df_t1.loc[df_t1.year == 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d3b4baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "2Daveraging True\n",
      "9 files desired\n",
      "0 files exist\n",
      "9 dates in original list\n",
      "9 dates to be written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "sample nan\n",
      "sample == np.nan\n",
      "len(brnch_dates) 9\n",
      "len(brnch_dates2) 18\n",
      "(3416, 2)\n",
      "6 dates bw 1980 and 1994\n",
      "12 dates bw 1995 and 2022\n",
      "(10, 2)\n",
      "i= 0\n",
      "era5_geopotential_2008_4_14.nc\n",
      "i= 1\n",
      "era5_geopotential_2008_4_15.nc\n",
      "i= 2\n",
      "era5_geopotential_2016_4_17.nc\n",
      "i= 3\n",
      "era5_geopotential_2016_4_18.nc\n",
      "i= 4\n",
      "era5_geopotential_2016_4_19.nc\n",
      "i= 5\n",
      "era5_geopotential_2016_4_20.nc\n",
      "i= 6\n",
      "era5_geopotential_2019_4_29.nc\n",
      "i= 7\n",
      "era5_geopotential_2019_4_30.nc\n",
      "i= 8\n",
      "era5_geopotential_2021_4_5.nc\n",
      "i= 9\n",
      "era5_geopotential_2021_4_6.nc\n",
      "ds w/ z dates b/w 95 and 22 is ready\n",
      "dates_1980_1994 [numpy.datetime64('1980-04-11T00:00:00.000000000'), numpy.datetime64('1988-04-04T00:00:00.000000000'), numpy.datetime64('1991-04-21T00:00:00.000000000'), numpy.datetime64('1980-04-12T00:00:00.000000000'), numpy.datetime64('1988-04-05T00:00:00.000000000'), numpy.datetime64('1991-04-22T00:00:00.000000000')]\n",
      "persist ds1 -- z dates b/w 1980 to 1994\n",
      "[########################################] | 100% Completed | 20m 27s\n",
      "ds1 is persisted\n",
      "['1980-04-11T00:00:00.000000000' '1980-04-12T00:00:00.000000000'\n",
      " '1988-04-04T00:00:00.000000000' '1988-04-05T00:00:00.000000000'\n",
      " '1991-04-21T00:00:00.000000000' '1991-04-22T00:00:00.000000000'\n",
      " '2008-04-14T00:00:00.000000000' '2008-04-15T00:00:00.000000000'\n",
      " '2016-04-17T00:00:00.000000000' '2016-04-18T00:00:00.000000000'\n",
      " '2016-04-19T00:00:00.000000000' '2016-04-20T00:00:00.000000000'\n",
      " '2019-04-29T00:00:00.000000000' '2019-04-30T00:00:00.000000000'\n",
      " '2021-04-05T00:00:00.000000000' '2021-04-06T00:00:00.000000000']\n",
      "Frozen({'isobaricInhPa': 9, 'latitude': 201, 'longitude': 481, 'date': 16})\n",
      "(5246, 2)\n",
      "6 dates bw 1980 and 1994\n",
      "12 dates bw 1995 and 2022\n",
      "(16, 2)\n",
      "i= 0\n",
      "era5_v_component_of_wind_1980_4_11.nc\n",
      "i= 1\n",
      "era5_v_component_of_wind_1980_4_12.nc\n",
      "i= 2\n",
      "era5_v_component_of_wind_1988_4_4.nc\n",
      "i= 3\n",
      "era5_v_component_of_wind_1988_4_5.nc\n",
      "i= 4\n",
      "era5_v_component_of_wind_1991_4_21.nc\n",
      "i= 5\n",
      "era5_v_component_of_wind_1991_4_22.nc\n",
      "i= 6\n",
      "era5_v_component_of_wind_2008_4_14.nc\n",
      "i= 7\n",
      "era5_v_component_of_wind_2008_4_15.nc\n",
      "i= 8\n",
      "era5_v_component_of_wind_2016_4_17.nc\n",
      "i= 9\n",
      "era5_v_component_of_wind_2016_4_18.nc\n",
      "i= 10\n",
      "era5_v_component_of_wind_2016_4_19.nc\n",
      "i= 11\n",
      "era5_v_component_of_wind_2016_4_20.nc\n",
      "i= 12\n",
      "era5_v_component_of_wind_2019_4_29.nc\n",
      "i= 13\n",
      "era5_v_component_of_wind_2019_4_30.nc\n",
      "i= 14\n",
      "era5_v_component_of_wind_2021_4_5.nc\n",
      "i= 15\n",
      "era5_v_component_of_wind_2021_4_6.nc\n",
      "(5246, 2)\n",
      "6 dates bw 1980 and 1994\n",
      "12 dates bw 1995 and 2022\n",
      "(16, 2)\n",
      "i= 0\n",
      "era5_u_component_of_wind_1980_4_11.nc\n",
      "i= 1\n",
      "era5_u_component_of_wind_1980_4_12.nc\n",
      "i= 2\n",
      "era5_u_component_of_wind_1988_4_4.nc\n",
      "i= 3\n",
      "era5_u_component_of_wind_1988_4_5.nc\n",
      "i= 4\n",
      "era5_u_component_of_wind_1991_4_21.nc\n",
      "i= 5\n",
      "era5_u_component_of_wind_1991_4_22.nc\n",
      "i= 6\n",
      "era5_u_component_of_wind_2008_4_14.nc\n",
      "i= 7\n",
      "era5_u_component_of_wind_2008_4_15.nc\n",
      "i= 8\n",
      "era5_u_component_of_wind_2016_4_17.nc\n",
      "i= 9\n",
      "era5_u_component_of_wind_2016_4_18.nc\n",
      "i= 10\n",
      "era5_u_component_of_wind_2016_4_19.nc\n",
      "i= 11\n",
      "era5_u_component_of_wind_2016_4_20.nc\n",
      "i= 12\n",
      "era5_u_component_of_wind_2019_4_29.nc\n",
      "i= 13\n",
      "era5_u_component_of_wind_2019_4_30.nc\n",
      "i= 14\n",
      "era5_u_component_of_wind_2021_4_5.nc\n",
      "i= 15\n",
      "era5_u_component_of_wind_2021_4_6.nc\n",
      "(5246, 2)\n",
      "6 dates bw 1980 and 1994\n",
      "12 dates bw 1995 and 2022\n",
      "(16, 2)\n",
      "i= 0\n",
      "era5_temperature_1980_4_11.nc\n",
      "i= 1\n",
      "era5_temperature_1980_4_12.nc\n",
      "i= 2\n",
      "era5_temperature_1988_4_4.nc\n",
      "i= 3\n",
      "era5_temperature_1988_4_5.nc\n",
      "i= 4\n",
      "era5_temperature_1991_4_21.nc\n",
      "i= 5\n",
      "era5_temperature_1991_4_22.nc\n",
      "i= 6\n",
      "era5_temperature_2008_4_14.nc\n",
      "i= 7\n",
      "era5_temperature_2008_4_15.nc\n",
      "i= 8\n",
      "era5_temperature_2016_4_17.nc\n",
      "i= 9\n",
      "era5_temperature_2016_4_18.nc\n",
      "i= 10\n",
      "era5_temperature_2016_4_19.nc\n",
      "i= 11\n",
      "era5_temperature_2016_4_20.nc\n",
      "i= 12\n",
      "era5_temperature_2019_4_29.nc\n",
      "i= 13\n",
      "era5_temperature_2019_4_30.nc\n",
      "i= 14\n",
      "era5_temperature_2021_4_5.nc\n",
      "i= 15\n",
      "era5_temperature_2021_4_6.nc\n",
      "['z_dly_clmt', 'z_dly_clmt_10D_roll']\n",
      "('clmt_roll_strftime', 'isobaricInhPa', 'latitude', 'longitude')\n",
      "['t_dly_clmt', 't_dly_clmt_10D_roll']\n",
      "('clmt_roll_strftime', 'isobaricInhPa', 'latitude', 'longitude')\n",
      "['u_dly_clmt', 'u_dly_clmt_10D_roll']\n",
      "('clmt_roll_strftime', 'isobaricInhPa', 'latitude', 'longitude')\n",
      "['v_dly_clmt', 'v_dly_clmt_10D_roll']\n",
      "('clmt_roll_strftime', 'isobaricInhPa', 'latitude', 'longitude')\n",
      "ds_brnch.dims Frozen({'latitude': 201, 'longitude': 481, 'date': 16})\n",
      "len(ds_brnch.date) 16\n",
      "len(ds_brnch2.date) 9\n",
      "-117.21494380223004\n",
      "-117.21494380223004\n",
      "Frozen({'date': 9, 'latitude': 201, 'longitude': 481})\n",
      "Data variables:\n",
      "    t              (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    t_anom_10D     (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    t_clmt         (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    u              (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    u_anom_10D     (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    u_clmt         (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    v              (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    v_anom_10D     (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    v_clmt         (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    z              (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    z_anom_10D     (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    z_clmt         (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    density        (date, latitude, longitude) float64 dask.array<chunksize=(9, 201, 481), meta=np.ndarray>\n",
      "    dse            (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    dse_anom       (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    dse_clmt       (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "    dse_anom_Lag1  (date, latitude, longitude) float64 dask.array<chunksize=(1, 201, 481), meta=np.ndarray>\n",
      "9 dates to be written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 0\n",
      "1980-04-12T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 327.60 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 1980-04-12\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_1980_04_12_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 1\n",
      "1988-04-05T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 205.13 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 1988-04-05\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_1988_04_05_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 2\n",
      "1991-04-22T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 206.96 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 1991-04-22\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_1991_04_22_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 3\n",
      "2019-04-30T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 205.88 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2019-04-30\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2019_04_30_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 4\n",
      "2016-04-18T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 307.91 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2016-04-18\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2016_04_18_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 5\n",
      "2021-04-06T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 312.39 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2021-04-06\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2021_04_06_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 6\n",
      "2016-04-20T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 306.95 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2016-04-20\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2016_04_20_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 7\n",
      "2008-04-15T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 206.75 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2008-04-15\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2008_04_15_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n",
      "i= 8\n",
      "2016-04-19T00:00:00.000000000\n",
      "[########################################] | 100% Completed | 206.54 ms\n",
      "Contains NaN values:\n",
      "<xarray.Dataset>\n",
      "Dimensions:        ()\n",
      "Coordinates:\n",
      "    date           datetime64[ns] 2016-04-19\n",
      "Data variables:\n",
      "    u              bool False\n",
      "    v              bool False\n",
      "    u_anom_10D     bool False\n",
      "    v_anom_10D     bool False\n",
      "    z_anom_10D     bool False\n",
      "    dse_anom       bool False\n",
      "    dse_anom_Lag1  bool False\n",
      "writing Apr_posbg_decay_nlsat_yNL_v2_2016_04_19_2Dave.nc\n",
      "9 dates have been written for Apr_posbg_decay_nlsat_yNL_v2.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(files)):\n",
    "    for two_day_averaging in [True]:\n",
    "        \n",
    "        print(i)\n",
    "        file = files[i]\n",
    "        print(file)\n",
    "        print('2Daveraging', two_day_averaging)\n",
    "\n",
    "        df = pd.read_csv(file, parse_dates=['date'])\n",
    "\n",
    "        desired_nc_lst = [\n",
    "            file.replace('.csv','') + '_' + str(df.date[j].date()).replace('-','_') \n",
    "            + '_2Dave.nc' # new add\n",
    "            for j in range(len(df.date))\n",
    "        ]\n",
    "        print(len(desired_nc_lst), 'files desired')\n",
    "\n",
    "        existing_nc_lst = [\n",
    "            s for s in os.listdir(dir_out) if \n",
    "            starts_with_substring_and_number(s, file.replace('.csv', '')+'_') \n",
    "            and s.endswith('_2Dave.nc') # new add\n",
    "        ]\n",
    "        print(len(existing_nc_lst), 'files exist')\n",
    "        \n",
    "#         break\n",
    "\n",
    "        #    skip iter if files exist \n",
    "        if set(existing_nc_lst) == set(desired_nc_lst):\n",
    "            print('index', i, 'already has all nc files in data folder')\n",
    "            continue\n",
    "\n",
    "#         if len(existing_nc_lst) > len(desired_nc_lst):\n",
    "#             nc_to_rmv_lst = [s for s in existing_nc_lst if s not in desired_nc_lst]\n",
    "#             for nc in nc_to_rmv_lst:\n",
    "#                 print('removing files', nc)\n",
    "#                 os.remove(dir_out + nc)\n",
    "#                 existing_nc_lst.remove(nc)\n",
    "\n",
    "#         desried_nc_updt = [s for s in desired_nc_lst if s not in existing_nc_lst]\n",
    "#         print('len(desried_nc_updt)', len(desried_nc_updt))\n",
    "\n",
    "        if 'Net_Adv' not in df.columns:\n",
    "            df['Net_Adv'] = df['adv_recon_2Dave'].copy()\n",
    "\n",
    "        flux_dts0 = df.date.to_list()\n",
    "        print(len(flux_dts0), 'dates in original list')\n",
    "#         print(len(existing_nc_lst), 'nc files available')\n",
    "\n",
    "#         flux_dts = [\n",
    "#             flux_dts0[i] for i in range(len(flux_dts0)) \n",
    "#             if file.replace('.csv','') + '_' + str(flux_dts0[i].date()).replace('-','_') + '.nc' \n",
    "#             in desried_nc_updt    \n",
    "#         ]\n",
    "\n",
    "        flux_dts = flux_dts0\n",
    "\n",
    "        if len(flux_dts)==0:\n",
    "            print('nothing to be written')\n",
    "            continue\n",
    "\n",
    "        print(len(flux_dts), 'dates to be written for', file)\n",
    "\n",
    "        read_files_comp(dates=flux_dts, sample = np.nan, two_day_averaging = two_day_averaging)\n",
    "\n",
    "        add_anoms()\n",
    "\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        \n",
    "        ds_brnch, ds_brnch2 = merge_branch_data(two_day_averaging=two_day_averaging)\n",
    "        print(len(ds_brnch2.date), 'dates to be written for', file)\n",
    "\n",
    "        for j in range(len(ds_brnch2.date)):\n",
    "            print('i=', j)\n",
    "            print(ds_brnch2.date.data[j])\n",
    "            if two_day_averaging == False:\n",
    "                file_out_str = file.replace('.csv','') + '_' + str(ds_brnch2.date.dt.date.data[j]).replace('-','_') + '.nc'\n",
    "            else:\n",
    "                file_out_str = file.replace('.csv','') + '_' + str(ds_brnch2.date.dt.date.data[j]).replace('-','_') + '_2Dave.nc'\n",
    "\n",
    "        #         if file_out_str in [s for s in os.listdir(dir_out) if s.startswith(file.replace('.csv',''))]:\n",
    "        #             continue\n",
    "\n",
    "            ## In case of 2D averaged ds, only dse_anom_Lag1 is not 2D aved;  everything else is 2D aved\n",
    "            with ProgressBar():\n",
    "                ds_out = ds_brnch2.isel(date=j).drop(['number','step','strftime','dates_roll_clmt'])\\\n",
    "                    [['u','v','u_anom_10D','v_anom_10D','z_anom_10D','dse_anom', 'dse_anom_Lag1']].compute()\n",
    "\n",
    "            # Check for NaN values\n",
    "            has_nan = ds_out.isnull().any()\n",
    "            print(\"Contains NaN values:\")\n",
    "            print(has_nan)\n",
    "            \n",
    "            if has_nan == False:\n",
    "                print('writing', file_out_str)\n",
    "            \n",
    "                if file_out_str in os.listdir(dir_out):\n",
    "                    os.remove(dir_out + file_out_str)\n",
    "                    \n",
    "                write_nc(\n",
    "                    ds = ds_out, \n",
    "                    dir_out = dir_out,\n",
    "                    file_out = file_out_str\n",
    "                )\n",
    "\n",
    "            print(len(ds_brnch2.date), 'dates have been written for', file)\n",
    "\n",
    "            #     del ds_brnch, ds_z_comp, ds_t_comp, ds_v_comp, ds_u_comp\n",
    "            gc.collect()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bad64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3154c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_conda",
   "language": "python",
   "name": "my_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
